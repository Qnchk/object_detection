{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8e67c67-10e4-4775-9ed9-77ed406891ca",
      "metadata": {
        "id": "b8e67c67-10e4-4775-9ed9-77ed406891ca"
      },
      "source": [
        "# Домашняя работа 3: Разработка модели детектирования объектов\n",
        "\n",
        "##  Погосян Арсен Андраникович\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a7555d9-939b-4482-aa39-74aa0044beb7",
      "metadata": {
        "id": "8a7555d9-939b-4482-aa39-74aa0044beb7"
      },
      "source": [
        "На семинаре мы обсудили, разработали и попробовали `SSD` модель на базе `VGG16`, поэтому для успешного выполнения домашнего задания рекомендуется обратиться к семинару.\n",
        "\n",
        "Задачи домашнего задания:\n",
        " - Загрузить набор данных и визуализировать объекты. (2 балла)\n",
        " - Разработать функцию для расчета метрики mAP задачи детектирования объектов. Продемонстрировать работу. (4 балла)\n",
        " - Натренировать `SSD` модель на базе VGG16. Продемонстрировать повышение метрики mAP (4 балла)\n",
        " - Разработать `SSD` модель согласно предлагаемой архитектуры на базе `ResNet18`. Продемонстрировать повышение метрики mAP (10 баллов)\n",
        " - (БОНУС) Добавить разнообразные аугментации изображений. Можно позаимствовать из других репозитариев с указанием источника. Повторить эксперименты (5 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "32683dba-de6d-484b-9fe0-68781006badd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32683dba-de6d-484b-9fe0-68781006badd",
        "outputId": "2d02557a-39f7-4518-9456-a73c8dafe0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'utils'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 24.75 KiB | 12.37 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SergeyMalashenko/ObjectDetectionProblemFromScratch.git utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d303b1c-6369-45c8-b124-0593d4c6770f",
      "metadata": {
        "id": "4d303b1c-6369-45c8-b124-0593d4c6770f"
      },
      "source": [
        "## Описание предлагаемых исходных кодов и набора данных\n",
        "- voc_dataset.py     - загружает предлагаемый набор данных, здесь также возможно выполнять аугментации над данными.\n",
        "- voc_dataloader.py  - формирует порцию данных.\n",
        "- prior_boxes.py     - содержит все необходимые функции для работы с `prior` боксами.\n",
        "    - prior_boxes - функция генерирует `prior` боксы в соотвествии с конфигурацией\n",
        "    - match       - функция решает задачу сопоставления `ground truth` боксов из набора данных и `prior` боксов\n",
        "    - decode      - функция вычисляет поправки между `ground truth` боксами и `prior` боксами, собственно их будет *предсказывать* наша модель\n",
        "    - encode      - функция пересчитывает `prior` боксы и *предсказанные* поправки в результирующие детектирования\n",
        "\n",
        "- multibox_loss.py - комплексная функция потерь, решается задача сопоставления *предсказанных* и `ground truth` боксов, применяется техника `hard-negative mining`, вычисляется общая функция потерь."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1fd76530-128e-47e8-b123-a484ab2ea7ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fd76530-128e-47e8-b123-a484ab2ea7ad",
        "outputId": "fe8e684c-73ba-4fdf-f330-90a0353bd8f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14UheyBtIByVktCsOR7OF2gBsTNGAw26E\n",
            "From (redirected): https://drive.google.com/uc?id=14UheyBtIByVktCsOR7OF2gBsTNGAw26E&confirm=t&uuid=b2f05b01-6345-4106-aaef-3ccb55e4b3ac\n",
            "To: /content/dataset.tar.gz\n",
            "100% 113M/113M [00:01<00:00, 96.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 14UheyBtIByVktCsOR7OF2gBsTNGAw26E\n",
        "!tar -xzf dataset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c5b243-819b-46da-9cd9-ede4dec9c204",
      "metadata": {
        "id": "d2c5b243-819b-46da-9cd9-ede4dec9c204"
      },
      "source": [
        "Набор данных состоит из `1528` изображений в разрешении `640x360` (тренировочная выборка - 1464 изображения, тестовая выборка - 64 изображения) в формате `VOC`.\n",
        "\n",
        "Набор имеет следующую структуру:\n",
        "\n",
        "|  Директория      | Содержимое |\n",
        "| ---------------- | ---------- |\n",
        "| Annotations      | папка содержит разметку в виде набора файлов в формате `XML`, каждый файл содержит информацию об объектах (класс объекта, рамка объекта `xmin,ymin,xmax,ymax`)    |\n",
        "| ImageSets/Main   | папка содержит два текстовых файла с распределением данных на тренирововчную `trainval.txt` и тестовую выборки `test.txt` |\n",
        "| JPEGImages       | папка содержит изображения в `JPEG` формате |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e6b401-d7e0-404c-8781-7ba3425768d7",
      "metadata": {
        "id": "17e6b401-d7e0-404c-8781-7ba3425768d7"
      },
      "source": [
        "## Архитектура модели на основе VGG16\n",
        "\n",
        "Обязательно посмотрите реализацию и разберитесь в том, как устроенна модель, это будет полезно при разработке собственной модели детектирования.\n",
        "\n",
        "![Image](output_vgg16.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623cacde-63b5-48f8-9939-48239381ad80",
      "metadata": {
        "id": "623cacde-63b5-48f8-9939-48239381ad80"
      },
      "source": [
        "## Архитектура модели на основе ResNet18\n",
        "\n",
        "Можно исследовать реализацию ResNet в пакете PyTorch, это будет полезно при разработке собственного детектора.\n",
        "\n",
        "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
        "\n",
        "![Image](output_resnet18.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "AtunQN_IU-4l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtunQN_IU-4l",
        "outputId": "58410bc6-e352-4f62-9c56-602bade5ed05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchview\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torchview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DO-QiloNiOa4",
      "metadata": {
        "id": "DO-QiloNiOa4"
      },
      "source": [
        "## Создаем конфигурацию, которая описывает работы нашего детектора (пример для VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "96aD0FFziUcG",
      "metadata": {
        "id": "96aD0FFziUcG"
      },
      "outputs": [],
      "source": [
        "#VGG16\n",
        "custom_config = {\n",
        " 'num_classes'  : 3,\n",
        " 'feature_maps' : [(45,80), (22,40), (11,20), (6,10), (4,8), (2,6)], #VGG16 - 640x360, размерность карт признаков, которые будут использоваться\n",
        " 'min_sizes'    : [0.10, 0.20, 0.37, 0.54, 0.71, 1.00], #Параметр масштаба боксов\n",
        " 'max_sizes'    : [0.20, 0.37, 0.54, 0.71, 1.00, 1.05], #Параметр масштаба боксов\n",
        "\n",
        " 'aspect_ratios': [[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]], #Список содержаший информацию о соотношении сторон для prior боксов\n",
        " 'num_priors'   : [6, 6, 6, 6, 4, 4], #Число prior боксов\n",
        " 'variance'     : [0.1, 0.2],\n",
        " 'clip'         :    True,\n",
        "\n",
        " 'overlap_threshold': 0.5, #Параметр IoU\n",
        " 'neg_pos_ratio'    :   3, #Параметр hard-negative mining\n",
        "\n",
        " 'model_name' : 'vgg16'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b3095f-8a4c-4231-be92-ff86f5df920d",
      "metadata": {
        "id": "97b3095f-8a4c-4231-be92-ff86f5df920d"
      },
      "source": [
        "## Загружаем требуемый набор данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f462dd9e-2b51-4386-9b26-1ead6d2c42eb",
      "metadata": {
        "id": "f462dd9e-2b51-4386-9b26-1ead6d2c42eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65d7233-8ed8-4d84-a96c-0d1baa8bd5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "#Добавляем директорию, чтобы была возможность импортировать необходимые модули\n",
        "sys.path.insert(0, 'utils')\n",
        "from utils.voc_dataloader import get_test_dataloader,get_train_dataloader\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "voc_root = \"dataset\"\n",
        "\n",
        "train_annotation_filename = os.path.join( voc_root, \"ImageSets/Main/trainval.txt\")\n",
        "test_annotation_filename  = os.path.join( voc_root, \"ImageSets/Main/test.txt\")\n",
        "\n",
        "train_dataloader = get_train_dataloader(voc_root, train_annotation_filename, 1, 1)\n",
        "test_dataloader  = get_test_dataloader(voc_root, test_annotation_filename, 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8902e8e-6068-4e6e-99a4-bf94136fe2e3",
      "metadata": {
        "id": "b8902e8e-6068-4e6e-99a4-bf94136fe2e3"
      },
      "source": [
        "## Разработать функцию вычисление mAP метрики задачи детектирования\n",
        "\n",
        "В качестве примера можно использовать https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c87fa0d3-334c-477f-89b6-1115a128cee0",
      "metadata": {
        "id": "c87fa0d3-334c-477f-89b6-1115a128cee0"
      },
      "outputs": [],
      "source": [
        "def compute_iou(pred_box, true_box):\n",
        "    # Формат box: [x_min, y_min, x_max, y_max]\n",
        "    x_min1, y_min1, x_max1, y_max1 = pred_box\n",
        "    x_min2, y_min2, x_max2, y_max2 = true_box\n",
        "\n",
        "    # Находим пересечение\n",
        "    inter_x_min = max(x_min1, x_min2)\n",
        "    inter_y_min = max(y_min1, y_min2)\n",
        "    inter_x_max = min(x_max1, x_max2)\n",
        "    inter_y_max = min(y_max1, y_max2)\n",
        "\n",
        "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
        "    area1 = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
        "    area2 = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
        "\n",
        "    union_area = area1 + area2 - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area > 0 else 0\n",
        "\n",
        "def calculate_ap(rec, prec):\n",
        "    # Вычисление Average Precision (AP)\n",
        "    rec = np.array(rec)\n",
        "    prec = np.array(prec)\n",
        "\n",
        "    # Добавим (0,0) и (1,1) для корректной интерполяции\n",
        "    mrec = np.concatenate(([0], rec, [1]))\n",
        "    mpre = np.concatenate(([0], prec, [0]))\n",
        "\n",
        "    # Интерполяция\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # Находим индекс первого элемента, равного 1\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "    return ap\n",
        "\n",
        "def mAP(custom_config, pred_boxes_all, pred_scores_all, gt_boxes_all, pred_labels_all, gt_labels_all, iou_threshold=0.5):\n",
        "    num_classes = custom_config['num_classes']\n",
        "    ap_per_class = []\n",
        "\n",
        "    # Перебираем каждый класс\n",
        "    for cls in range(num_classes):\n",
        "        # Собираем все предсказания для этого класса\n",
        "        cls_pred_boxes = []\n",
        "        cls_pred_scores = []\n",
        "        cls_pred_labels = []\n",
        "        cls_gt_boxes = []\n",
        "        cls_gt_labels = []\n",
        "\n",
        "        for i in range(len(gt_boxes_all)):\n",
        "            gt_boxes = gt_boxes_all[i]\n",
        "            gt_labels = gt_labels_all[i]\n",
        "\n",
        "            # Для каждого изображения добавляем боксы и метки этого класса\n",
        "            for j in range(len(gt_labels)):\n",
        "                if gt_labels[j] == cls:\n",
        "                    cls_gt_boxes.append(gt_boxes[j])\n",
        "                    cls_gt_labels.append(gt_labels[j])\n",
        "\n",
        "            # Добавляем предсказания для этого изображения\n",
        "            if i < len(pred_labels_all):\n",
        "                pred_labels = pred_labels_all[i]\n",
        "                pred_boxes = pred_boxes_all[i]\n",
        "                pred_scores = pred_scores_all[i]\n",
        "\n",
        "                for j in range(len(pred_labels)):\n",
        "                    if j < len(pred_boxes) and j < len(pred_scores):\n",
        "                        if pred_labels[j] == cls:\n",
        "                            cls_pred_boxes.append(pred_boxes[j])\n",
        "                            cls_pred_scores.append(pred_scores[j])\n",
        "                            cls_pred_labels.append(pred_labels[j])\n",
        "\n",
        "        if len(cls_pred_boxes) == 0:\n",
        "            continue\n",
        "\n",
        "        # Сортируем предсказания по уверенности\n",
        "        sorted_idx = np.argsort(cls_pred_scores)[::-1]\n",
        "        cls_pred_boxes = np.array(cls_pred_boxes)[sorted_idx]\n",
        "        cls_pred_scores = np.array(cls_pred_scores)[sorted_idx]\n",
        "        cls_pred_labels = np.array(cls_pred_labels)[sorted_idx]\n",
        "\n",
        "        tp = np.zeros(len(cls_pred_boxes))\n",
        "        fp = np.zeros(len(cls_pred_boxes))\n",
        "        total_true = len(cls_gt_boxes)\n",
        "\n",
        "        detected = []  # Этот список будет хранить индексы уже обнаруженных true_boxes\n",
        "\n",
        "        for i, pred_box in enumerate(cls_pred_boxes):\n",
        "            best_iou = 0\n",
        "            best_true_box = None\n",
        "            for j, true_box in enumerate(cls_gt_boxes):\n",
        "                # Используем IoU для проверки пересечения с уже обнаруженными боками\n",
        "                iou = compute_iou(pred_box, true_box)\n",
        "                if iou > best_iou and j not in detected:  # Проверяем, что true_box не был ранее найден\n",
        "                    best_iou = iou\n",
        "                    best_true_box = true_box\n",
        "                    best_true_box_idx = j  # Индекс найденного true_box\n",
        "\n",
        "            if best_iou >= iou_threshold:\n",
        "                tp[i] = 1\n",
        "                detected.append(best_true_box_idx)  # Добавляем индекс true_box в список обнаруженных\n",
        "            else:\n",
        "                fp[i] = 1\n",
        "\n",
        "        # Рассчитываем Precision и Recall\n",
        "        tp_cumsum = np.cumsum(tp)\n",
        "        fp_cumsum = np.cumsum(fp)\n",
        "\n",
        "        recall = tp_cumsum / total_true if total_true > 0 else np.zeros_like(tp_cumsum)\n",
        "        precision = tp_cumsum / (tp_cumsum + fp_cumsum) if (tp_cumsum + fp_cumsum).sum() > 0 else np.zeros_like(tp_cumsum)\n",
        "\n",
        "        # Вычисляем Average Precision (AP) для этого класса\n",
        "        ap = calculate_ap(recall, precision)\n",
        "        ap_per_class.append(ap)\n",
        "\n",
        "    # Вычисляем средний AP по всем классам (mAP)\n",
        "    map_score = np.mean(ap_per_class) if len(ap_per_class) > 0 else 0\n",
        "    return map_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02651df0-5433-407c-b1e3-b702d3920a1f",
      "metadata": {
        "id": "02651df0-5433-407c-b1e3-b702d3920a1f"
      },
      "source": [
        "## Натренировать модель на основе VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2df2be84-327d-41db-af9d-6fbc2f308cfa",
      "metadata": {
        "id": "2df2be84-327d-41db-af9d-6fbc2f308cfa"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import VGG16_Weights\n",
        "from torchvision        import models\n",
        "from torchview          import draw_graph\n",
        "from torch              import nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, n_channels, scale):\n",
        "        super(L2Norm, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.gamma      = scale or None\n",
        "        self.eps        = 1e-10\n",
        "        self.weight     = nn.Parameter(torch.Tensor(self.n_channels))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.constant_(self.weight, self.gamma)\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.sqrt(x.pow(2).sum(dim=1, keepdim=True)) + self.eps\n",
        "        x = torch.div(x, norm)\n",
        "        x = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n",
        "        return x\n",
        "\n",
        "class SSD_VGG16(nn.Module):\n",
        "    def __init__(self, num_bboxes_s, num_labels = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_bboxes_s = num_bboxes_s\n",
        "        self.num_labels   = num_labels\n",
        "\n",
        "        self.used_layer_id_s       = [21, 33, 37, 41, 45, 49] #\n",
        "        self.norm_layer            = L2Norm(512, 20)\n",
        "\n",
        "        base_layers       = self._build_base_layers ()\n",
        "        extra_layers      = self._build_extra_layers()\n",
        "        self.total_layers = base_layers + extra_layers\n",
        "\n",
        "        self.conf_layers, self.loc_layers = self._build_conf_loc_layers()\n",
        "\n",
        "    def _build_base_layers(self):\n",
        "        backbone_model    = models.vgg16(weights=VGG16_Weights.DEFAULT)  #False\n",
        "\n",
        "        base_layers = nn.ModuleList(list(backbone_model.features)[:-1])\n",
        "        base_layers[16].ceil_mode = True\n",
        "\n",
        "        pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        conv6 = nn.Conv2d( 512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "        relu6 = nn.ReLU(inplace=True)\n",
        "        conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        relu7 = nn.ReLU(inplace=True)\n",
        "\n",
        "        nn.init.xavier_uniform_(conv6.weight)\n",
        "        nn.init.zeros_         (conv6  .bias)\n",
        "        nn.init.xavier_uniform_(conv7.weight)\n",
        "        nn.init.zeros_         (conv7  .bias)\n",
        "\n",
        "        base_layers.extend( [pool5, conv6, relu6, conv7, relu7] )\n",
        "\n",
        "        return base_layers\n",
        "\n",
        "    def _build_extra_layers(self):\n",
        "        extra_layers = []\n",
        "\n",
        "        conv8_1  = nn.Conv2d( 1024, 256, kernel_size=1, stride=1           )\n",
        "        relu8_1  = nn.ReLU(inplace=True)\n",
        "        conv8_2  = nn.Conv2d( 256, 512, kernel_size=3, stride=2, padding=1)\n",
        "        relu8_2  = nn.ReLU(inplace=True)\n",
        "        conv9_1  = nn.Conv2d( 512, 128, kernel_size=1, stride=1           )\n",
        "        relu9_1  = nn.ReLU(inplace=True)\n",
        "        conv9_2  = nn.Conv2d( 128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        relu9_2  = nn.ReLU(inplace=True)\n",
        "        conv10_1 = nn.Conv2d( 256, 128, kernel_size=1, stride=1           )\n",
        "        relu10_1 = nn.ReLU(inplace=True)\n",
        "        conv10_2 = nn.Conv2d( 128, 256, kernel_size=3, stride=1           )\n",
        "        relu10_2 = nn.ReLU(inplace=True)\n",
        "        conv11_1 = nn.Conv2d( 256, 128, kernel_size=1                     )\n",
        "        relu11_1 = nn.ReLU(inplace=True)\n",
        "        conv11_2 = nn.Conv2d( 128, 256, kernel_size=3, stride=1           )\n",
        "        relu11_2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        nn.init.xavier_uniform_(conv8_1 .weight)\n",
        "        nn.init.zeros_         (conv8_1 .bias  )\n",
        "        nn.init.xavier_uniform_(conv8_2 .weight)\n",
        "        nn.init.zeros_         (conv8_2 .bias  )\n",
        "        nn.init.xavier_uniform_(conv9_1 .weight)\n",
        "        nn.init.zeros_         (conv9_1 .bias  )\n",
        "        nn.init.xavier_uniform_(conv9_2 .weight)\n",
        "        nn.init.zeros_         (conv9_2 .bias  )\n",
        "        nn.init.xavier_uniform_(conv10_1.weight)\n",
        "        nn.init.zeros_         (conv10_1.bias  )\n",
        "        nn.init.xavier_uniform_(conv10_2.weight)\n",
        "        nn.init.zeros_         (conv10_2.bias  )\n",
        "        nn.init.xavier_uniform_(conv11_1.weight)\n",
        "        nn.init.zeros_         (conv11_1.bias  )\n",
        "        nn.init.xavier_uniform_(conv11_2.weight)\n",
        "        nn.init.zeros_         (conv11_2.bias  )\n",
        "\n",
        "        extra_layers = nn.ModuleList( [conv8_1, relu8_1, conv8_2, relu8_2, conv9_1, relu9_1, conv9_2, relu9_2, conv10_1, relu10_1, conv10_2, relu10_2, conv11_1, relu11_1, conv11_2, relu11_2] )\n",
        "        return extra_layers\n",
        "\n",
        "    def _build_conf_loc_layers(self):\n",
        "        out_channels_s = [ self.total_layers[i].out_channels for i in self.used_layer_id_s ]\n",
        "\n",
        "        conf_layers, loc_layers = [], []\n",
        "        for i, j in enumerate(self.used_layer_id_s):\n",
        "            conf_layer = nn.Conv2d( self.total_layers[j].out_channels, self.num_bboxes_s[i] * self.num_labels, kernel_size=3, padding=1)\n",
        "            loc_layer  = nn.Conv2d( self.total_layers[j].out_channels, self.num_bboxes_s[i] * 4              , kernel_size=3, padding=1)\n",
        "\n",
        "            nn.init.xavier_uniform_(conf_layer.weight)\n",
        "            nn.init.zeros_         (conf_layer  .bias)\n",
        "            nn.init.xavier_uniform_(loc_layer .weight)\n",
        "            nn.init.zeros_         (loc_layer   .bias)\n",
        "\n",
        "            conf_layers += [conf_layer]\n",
        "            loc_layers  += [loc_layer ]\n",
        "\n",
        "        conf_layers = nn.ModuleList(conf_layers)\n",
        "        loc_layers  = nn.ModuleList(loc_layers )\n",
        "\n",
        "        return conf_layers, loc_layers\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "        source_s, loc_s, conf_s = [], [], []\n",
        "\n",
        "        for i, current_layer in enumerate(self.total_layers, -1):\n",
        "            x = current_layer(x)\n",
        "            if i in self.used_layer_id_s:\n",
        "                if i == 21:\n",
        "                    s = self.norm_layer(x)\n",
        "                else:\n",
        "                    s = x\n",
        "                source_s.append(s)\n",
        "        for s, l, c in zip(source_s, self.loc_layers, self.conf_layers):\n",
        "            conf_s.append(c(s).permute(0, 2, 3, 1).contiguous())\n",
        "            loc_s .append(l(s).permute(0, 2, 3, 1).contiguous())\n",
        "        conf_s = torch.cat([o.view(o.size(0), -1) for o in conf_s], 1)\n",
        "        loc_s  = torch.cat([o.view(o.size(0), -1) for o in loc_s ], 1)\n",
        "\n",
        "        conf_s = conf_s.view(conf_s.size(0), -1, self.num_labels)\n",
        "        loc_s  = loc_s .view(loc_s .size(0), -1, 4              )\n",
        "\n",
        "        return loc_s, conf_s\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.prior_boxes import prior_boxes, match, encode, decode\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD Weighted Loss Function\n",
        "    Compute Targets:\n",
        "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
        "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
        "           (default threshold: 0.5).\n",
        "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
        "           truth boxes and their matched  'priorboxes'.\n",
        "        3) Hard negative mining to filter the excessive number of negative examples\n",
        "           that comes with using a large number of default bounding boxes.\n",
        "           (default negative:positive ratio 3:1)\n",
        "    Objective Loss:\n",
        "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
        "        weighted by α which is set to 1 by cross val.\n",
        "        Args:\n",
        "            c: class confidences,\n",
        "            l: predicted boxes,\n",
        "            g: ground truth boxes\n",
        "            N: number of matched default boxes\n",
        "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, overlap_threshold, neg_pos_ratio, variance):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.threshold     = overlap_threshold\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.variance      = variance\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"Multibox Loss\n",
        "        Args:\n",
        "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
        "            and prior boxes from SSD net.\n",
        "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
        "                loc shape: torch.size(batch_size,num_priors,4)\n",
        "                priors shape: torch.size(num_priors,4)\n",
        "\n",
        "            targets (tensor): Ground truth boxes and labels for a batch,\n",
        "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
        "        \"\"\"\n",
        "        loc_data, conf_data, priors = predictions\n",
        "        gt_label_s, gt_box_s = targets\n",
        "\n",
        "        device = loc_data.device\n",
        "\n",
        "        batch_size  = loc_data .size( 0)\n",
        "        num_priors  = loc_data .size( 1)\n",
        "        num_classes = conf_data.size(-1)\n",
        "\n",
        "        # match priors (default boxes) and ground truth boxes\n",
        "        loc_t  = torch.zeros(batch_size, num_priors, 4, device=device).float()\n",
        "        conf_t = torch.zeros(batch_size, num_priors   , device=device).long ()\n",
        "\n",
        "        neg_pos_ratio = self.neg_pos_ratio\n",
        "        threshold     = self.threshold\n",
        "        variance      = self.variance\n",
        "        #Решаем задачу соответствия между GT боксами и предсказанными боксами\n",
        "        for idx in range(batch_size):\n",
        "            loc_t[idx], conf_t[idx] = match(threshold, gt_box_s[idx], priors, variance, gt_label_s[idx])\n",
        "\n",
        "        pos = conf_t > 0\n",
        "        num_pos = pos.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # Вычисляем Localization Loss (Smooth L1)\n",
        "        # Shape: [batch, num_priors, 4]\n",
        "        loc_p = loc_data[pos].view(-1, 4)\n",
        "        loc_t = loc_t   [pos].view(-1, 4)\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # Вычисляем Classification Loss\n",
        "        loss_c = F.cross_entropy(conf_data.view(-1, num_classes), conf_t.view(-1), reduction='none')\n",
        "        loss_c = loss_c.view(batch_size, num_priors)\n",
        "\n",
        "        # Filter out the negative samples and reduce the loss by sum\n",
        "        loss_c_pos = loss_c[pos].sum()\n",
        "\n",
        "        # Hard negative mining\n",
        "        num_neg = torch.clamp(neg_pos_ratio * num_pos, max=pos.size(1) - 1)\n",
        "        loss_c_neg = loss_c * ~pos\n",
        "        loss_c_neg, _ = loss_c_neg.sort(1, descending=True)\n",
        "        neg_mask = torch.zeros_like(loss_c_neg)\n",
        "        neg_mask[torch.arange(batch_size), num_neg.view(-1)] = 1.\n",
        "        neg_mask = 1 - neg_mask.cumsum(-1)\n",
        "        loss_c_neg = (loss_c_neg * neg_mask).sum()\n",
        "\n",
        "        # Finally we normalize the losses by the number of positives\n",
        "        N = num_pos.sum()\n",
        "        loss_l = loss_l / N\n",
        "        loss_c = (loss_c_pos + loss_c_neg) / N\n",
        "\n",
        "        return loss_l, loss_c"
      ],
      "metadata": {
        "id": "eEpmM_i29rTc"
      },
      "id": "eEpmM_i29rTc",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from utils.voc_dataloader     import get_train_dataloader, get_test_dataloader\n",
        "from utils.prior_boxes        import detect_objects, prior_boxes\n",
        "\n",
        "from tqdm        import tqdm\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def train_process(args, custom_config):\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(seed=args.seed)\n",
        "\n",
        "    dataset_root_dir = args.dataset_root_dir\n",
        "    train_annotation_filename = os.path.join(dataset_root_dir, \"ImageSets/Main/trainval.txt\")\n",
        "    test_annotation_filename = os.path.join(dataset_root_dir, \"ImageSets/Main/test.txt\")\n",
        "    train_dataloader = get_train_dataloader(dataset_root_dir, train_annotation_filename, args.batch_size, args.num_workers)\n",
        "    test_dataloader = get_test_dataloader(dataset_root_dir, test_annotation_filename, args.batch_size, args.num_workers)\n",
        "\n",
        "    learning_rate = args.learning_rate\n",
        "\n",
        "    if not os.path.exists(args.output):\n",
        "        os.mkdir(args.output)\n",
        "\n",
        "    model = SSD_VGG16(custom_config['num_priors'], custom_config['num_classes'])\n",
        "\n",
        "    prior_box_s = prior_boxes(custom_config)\n",
        "    prior_box_s_gpu = prior_box_s.cuda()\n",
        "\n",
        "    overlap_threshold = custom_config['overlap_threshold']\n",
        "    neg_pos_ratio = custom_config['neg_pos_ratio']\n",
        "    variance = custom_config['variance']\n",
        "\n",
        "    criterion = MultiBoxLoss(overlap_threshold, neg_pos_ratio, variance)\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = MultiStepLR(optimizer=optimizer, milestones=args.multistep, gamma=0.2)\n",
        "\n",
        "    best_loc_loss, best_cls_loss, best_loss = np.inf, np.inf, np.inf\n",
        "    train_loss_s, eval_loss_s = [], []\n",
        "    for epoch in tqdm(list(range(args.epochs))):\n",
        "        # Train model\n",
        "        train_loc_loss, train_cls_loss, train_loss = 0, 0, 0\n",
        "        model.train()\n",
        "        for i, (image_s_cpu, box_ss_cpu, label_ss_cpu) in enumerate(train_dataloader):\n",
        "            if len(box_ss_cpu) > 0 and len(label_ss_cpu) > 0:\n",
        "                image_s_gpu = image_s_cpu.cuda()\n",
        "                label_ss_gpu = [label_s_cpu.cuda() for label_s_cpu in label_ss_cpu]\n",
        "                box_ss_gpu = [box_s_cpu.cuda() for box_s_cpu in box_ss_cpu]\n",
        "\n",
        "                pred_loc_ss_gpu, pred_conf_ss_gpu = model(image_s_gpu)\n",
        "\n",
        "                loc_loss, cls_loss = criterion(\n",
        "                    (pred_loc_ss_gpu, pred_conf_ss_gpu, prior_box_s_gpu), (label_ss_gpu, box_ss_gpu)\n",
        "                )\n",
        "\n",
        "                loss = loc_loss + cls_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loc_loss += loc_loss.item()\n",
        "                train_cls_loss += cls_loss.item()\n",
        "                train_loss += loss.item()\n",
        "        scheduler.step()\n",
        "        train_loss_s.append(train_loss)\n",
        "\n",
        "        # Eval model\n",
        "        eval_loc_loss, eval_cls_loss, eval_loss = 0, 0, 0\n",
        "        model.eval()\n",
        "\n",
        "        # Data for mAP\n",
        "        pred_boxes_all, pred_scores_all, pred_labels_all = [], [], []\n",
        "        gt_boxes_all, gt_labels_all = [], []\n",
        "\n",
        "        for i, (image_s_cpu, box_ss_cpu, label_ss_cpu) in enumerate(test_dataloader):\n",
        "            if len(box_ss_cpu) > 0 and len(label_ss_cpu) > 0:\n",
        "                image_s_gpu = image_s_cpu.cuda()\n",
        "                label_ss_gpu = [label_s_cpu.cuda() for label_s_cpu in label_ss_cpu]\n",
        "                box_ss_gpu = [box_s_cpu.cuda() for box_s_cpu in box_ss_cpu]\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    pred_loc_ss_gpu, pred_conf_ss_gpu = model(image_s_gpu)\n",
        "\n",
        "                # Convert predictions to boxes, scores, and labels\n",
        "                pred_boxes, pred_labels, pred_scores = detect_objects(\n",
        "                    pred_loc_ss_gpu.cpu(), pred_conf_ss_gpu.cpu(), prior_box_s,\n",
        "                    custom_config['num_classes'], 0.5, 0.3\n",
        "                )\n",
        "\n",
        "                pred_boxes_all.extend(pred_boxes)\n",
        "                pred_scores_all.extend(pred_scores)\n",
        "                pred_labels_all.extend(pred_labels)\n",
        "                gt_boxes_all.extend(box_ss_cpu)\n",
        "                gt_labels_all.extend(label_ss_cpu)\n",
        "\n",
        "                # Calculate loss for evaluation\n",
        "                loc_loss, cls_loss = criterion(\n",
        "                    (pred_loc_ss_gpu, pred_conf_ss_gpu, prior_box_s_gpu), (label_ss_gpu, box_ss_gpu)\n",
        "                )\n",
        "                loss = loc_loss + cls_loss\n",
        "\n",
        "                eval_loc_loss += loc_loss.item()\n",
        "                eval_cls_loss += cls_loss.item()\n",
        "                eval_loss += loss.item()\n",
        "        eval_loss_s.append(eval_loss)\n",
        "\n",
        "\n",
        "        map_score = mAP(custom_config, pred_boxes_all, pred_scores_all, gt_boxes_all,pred_labels, gt_labels_all)\n",
        "\n",
        "\n",
        "        print(\n",
        "            f'epoch[{epoch}] | lr {scheduler.get_last_lr()[0]:.5f} | '\n",
        "            f'loc_loss [{train_loc_loss:.2f}/{eval_loc_loss:.2f}] | '\n",
        "            f'cls_loss [{train_cls_loss:.2f}/{eval_cls_loss:.2f}] | '\n",
        "            f'total_loss [{train_loss:.2f}/{eval_loss:.2f}] | mAP {map_score:.4f}'\n",
        "        )\n",
        "\n",
        "        if eval_loss < best_loss:\n",
        "            torch.save(model.state_dict(), os.path.join(args.output, f\"{custom_config['model_name']}.pth\"))\n",
        "            best_loc_loss, best_cls_loss, best_loss = eval_loc_loss, eval_cls_loss, eval_loss\n",
        "\n",
        "    return model, prior_box_s, train_loss_s, eval_loss_s\n",
        "\n"
      ],
      "metadata": {
        "id": "qxojPIV-4Mm-"
      },
      "id": "qxojPIV-4Mm-",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "param_s = Namespace(\n",
        " dataset_root_dir='dataset',\n",
        " epochs = 15, batch_size = 8,\n",
        " checkpoint = None, output = 'output',\n",
        " multistep = [20, 30, 40],\n",
        " learning_rate = 1e-3, momentum = 0.9,\n",
        " weight_decay = 0.0005, warmup = None,\n",
        " num_workers = 4,\n",
        " seed = 42\n",
        ")\n",
        "\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "custom_config = {\n",
        " 'num_classes'  : 2,\n",
        " 'feature_maps' : [(45,80), (22,40), (11,20), (6,10), (4,8), (2,6)], #VGG16 - 640x360\n",
        "\n",
        " 'min_sizes'    : [0.10, 0.20, 0.37, 0.54, 0.71, 1.00],\n",
        " 'max_sizes'    : [0.20, 0.37, 0.54, 0.71, 1.00, 1.05],\n",
        "\n",
        " 'aspect_ratios': [[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        " 'num_priors'   : [6, 6, 6, 6, 4, 4],\n",
        " 'variance'     : [0.1, 0.2],\n",
        " 'clip'         :    True,\n",
        "\n",
        " 'overlap_threshold': 0.5,\n",
        " 'neg_pos_ratio'    :   3,\n",
        "\n",
        " 'model_name' : 'vgg16'\n",
        "}\n",
        "\n",
        "model, prior_box_s, train_loss_s, eval_loss_s = train_process(param_s, custom_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATL9AmFy5Pj9",
        "outputId": "83d51365-2ad2-4e37-9093-98d440f3c15f"
      },
      "id": "ATL9AmFy5Pj9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[0] | lr 0.00100 | loc_loss [388.40/14.53] | cls_loss [1059.02/65.49] | total_loss [1447.42/80.02] | mAP 0.0387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [02:15<31:35, 135.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[1] | lr 0.00100 | loc_loss [225.22/7.46] | cls_loss [785.65/14.69] | total_loss [1010.88/22.15] | mAP 0.0941\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [04:30<29:18, 135.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[2] | lr 0.00100 | loc_loss [159.59/5.64] | cls_loss [396.99/13.11] | total_loss [556.58/18.74] | mAP 0.0958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [06:46<27:04, 135.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[3] | lr 0.00100 | loc_loss [132.39/4.73] | cls_loss [282.98/10.87] | total_loss [415.38/15.60] | mAP 0.0988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [09:02<24:52, 135.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[4] | lr 0.00100 | loc_loss [116.38/4.55] | cls_loss [254.71/10.55] | total_loss [371.09/15.10] | mAP 0.0983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [11:18<22:38, 135.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[5] | lr 0.00100 | loc_loss [101.58/4.16] | cls_loss [239.76/10.37] | total_loss [341.34/14.54] | mAP 0.1061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [13:35<20:25, 136.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[6] | lr 0.00100 | loc_loss [92.81/3.65] | cls_loss [228.21/9.55] | total_loss [321.02/13.20] | mAP 0.1086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [15:52<18:12, 136.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[7] | lr 0.00100 | loc_loss [85.26/3.41] | cls_loss [218.10/9.09] | total_loss [303.35/12.50] | mAP 0.1040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [18:08<15:53, 136.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[8] | lr 0.00100 | loc_loss [78.46/2.99] | cls_loss [208.40/8.50] | total_loss [286.87/11.49] | mAP 0.1059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 10/15 [22:40<11:20, 136.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[9] | lr 0.00100 | loc_loss [74.55/3.01] | cls_loss [200.19/9.37] | total_loss [274.74/12.39] | mAP 0.1149\n",
            "epoch[10] | lr 0.00100 | loc_loss [69.83/2.69] | cls_loss [194.93/8.36] | total_loss [264.76/11.05] | mAP 0.1152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 12/15 [27:12<06:48, 136.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[11] | lr 0.00100 | loc_loss [66.49/2.75] | cls_loss [188.38/8.59] | total_loss [254.87/11.35] | mAP 0.1170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [29:28<04:32, 136.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[12] | lr 0.00100 | loc_loss [64.49/2.64] | cls_loss [188.05/8.51] | total_loss [252.54/11.15] | mAP 0.1118\n",
            "epoch[13] | lr 0.00100 | loc_loss [61.77/2.55] | cls_loss [186.27/8.37] | total_loss [248.04/10.91] | mAP 0.1111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [31:46<02:16, 136.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[14] | lr 0.00100 | loc_loss [59.63/2.62] | cls_loss [179.64/7.95] | total_loss [239.27/10.57] | mAP 0.1006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [34:02<00:00, 136.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ee17f9-c4fc-4656-a0ba-76f6bbc8bea2",
      "metadata": {
        "id": "a6ee17f9-c4fc-4656-a0ba-76f6bbc8bea2"
      },
      "source": [
        "## Разработать модель на основе ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bbe6cfed-d546-459a-aba0-ccb3035a418e",
      "metadata": {
        "id": "bbe6cfed-d546-459a-aba0-ccb3035a418e"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchvision.models import ResNet18_Weights\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BaseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,  operator=None, strd=1):\n",
        "        super(BaseBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.operator = operator\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=strd, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=strd, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.operator is not None:\n",
        "            identity = self.operator(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ExtraBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ExtraBlock, self).__init__()\n",
        "        self.out_channels = in_channels\n",
        "        self.conv1  = nn.Conv2d( in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.relu1  = nn.ReLU(inplace=True)\n",
        "        self.conv2  = nn.Conv2d( out_channels, in_channels, kernel_size=3, stride=2, padding=1)\n",
        "        self.relu2  = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class resnet18(nn.Module):\n",
        "    def __init__(self, num_bboxes_s, num_labels = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_bboxes_s = num_bboxes_s\n",
        "        self.num_labels   = num_labels\n",
        "\n",
        "        self.used_layer_id_s       = [7, 8, 9, 10, 11, 12] #\n",
        "\n",
        "        base_layers       = self._build_base_layers ()\n",
        "        extra_layers      = self._build_extra_layers()\n",
        "        self.total_layers = base_layers + extra_layers\n",
        "\n",
        "        self.conf_layers, self.loc_layers = self._build_conf_loc_layers()\n",
        "\n",
        "    def _build_base_layers(self):\n",
        "        backbone_model    = models.resnet18(weights=ResNet18_Weights.DEFAULT)  #False\n",
        "\n",
        "        base_layers = nn.ModuleList(list(backbone_model.children())[:-3])\n",
        "        base_block_1 = BaseBlock(in_channels=256, out_channels=128, operator = nn.Sequential(nn.Conv2d(256, 128, kernel_size=(1,1), stride=(1,1), bias=False), nn.BatchNorm2d(128)))\n",
        "        base_block_2 = BaseBlock(in_channels=128, out_channels=128)\n",
        "\n",
        "        base_layers.extend( [base_block_1, base_block_2] )\n",
        "        return base_layers\n",
        "\n",
        "    def _build_extra_layers(self):\n",
        "        extra_layers = []\n",
        "\n",
        "        extra_block1 = ExtraBlock(128, 64)\n",
        "        extra_block2 = ExtraBlock(128, 64)\n",
        "        extra_block3 = ExtraBlock(128, 64)\n",
        "        extra_block4 = ExtraBlock(128, 64)\n",
        "        extra_block5 = ExtraBlock(128, 64)\n",
        "        extra_layers = nn.ModuleList( [extra_block1,\n",
        "                                       extra_block2,\n",
        "                                       extra_block3,\n",
        "                                       extra_block4,\n",
        "                                       extra_block5, ] )\n",
        "        return extra_layers\n",
        "\n",
        "    def _build_conf_loc_layers(self):\n",
        "        out_channels_s = [ self.total_layers[i].out_channels for i in self.used_layer_id_s ]\n",
        "\n",
        "        conf_layers, loc_layers = [], []\n",
        "        for i, j in enumerate(self.used_layer_id_s):\n",
        "            conf_layer = nn.Conv2d( self.total_layers[j].out_channels, self.num_bboxes_s[i] * self.num_labels, kernel_size=3, padding=1)\n",
        "            loc_layer  = nn.Conv2d( self.total_layers[j].out_channels, self.num_bboxes_s[i] * 4              , kernel_size=3, padding=1)\n",
        "\n",
        "            conf_layers += [conf_layer]\n",
        "            loc_layers  += [loc_layer ]\n",
        "\n",
        "        conf_layers = nn.ModuleList(conf_layers)\n",
        "        loc_layers  = nn.ModuleList(loc_layers )\n",
        "\n",
        "        return conf_layers, loc_layers\n",
        "\n",
        "    def forward(self, x, verbose=False):\n",
        "        source_s, loc_s, conf_s = [], [], []\n",
        "        for i, current_layer in enumerate(self.total_layers, -1):\n",
        "            x = current_layer(x)\n",
        "            if i in self.used_layer_id_s:\n",
        "                s = x\n",
        "                source_s.append(s)\n",
        "        for s, l, c in zip(source_s, self.loc_layers, self.conf_layers):\n",
        "            conf_s.append(c(s).permute(0, 2, 3, 1).contiguous())\n",
        "            loc_s .append(l(s).permute(0, 2, 3, 1).contiguous())\n",
        "        conf_s = torch.cat([o.view(o.size(0), -1) for o in conf_s], 1)\n",
        "        loc_s  = torch.cat([o.view(o.size(0), -1) for o in loc_s ], 1)\n",
        "\n",
        "        conf_s = conf_s.view(conf_s.size(0), -1, self.num_labels)\n",
        "        loc_s  = loc_s .view(loc_s .size(0), -1, 4              )\n",
        "        return loc_s, conf_s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_rCLA6gvKVB"
      },
      "id": "H_rCLA6gvKVB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ce233b55-a933-4e95-99f3-8a9a8540afa5",
      "metadata": {
        "id": "ce233b55-a933-4e95-99f3-8a9a8540afa5"
      },
      "source": [
        "## Натренировать модель на основе ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_resnet(args, custom_config):\n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(seed=args.seed)\n",
        "\n",
        "    dataset_root_dir = args.dataset_root_dir\n",
        "    train_annotation_filename = os.path.join(dataset_root_dir, \"ImageSets/Main/trainval.txt\")\n",
        "    test_annotation_filename = os.path.join(dataset_root_dir, \"ImageSets/Main/test.txt\")\n",
        "    train_dataloader = get_train_dataloader(dataset_root_dir, train_annotation_filename, args.batch_size, args.num_workers)\n",
        "    test_dataloader = get_test_dataloader(dataset_root_dir, test_annotation_filename, args.batch_size, args.num_workers)\n",
        "\n",
        "    learning_rate = args.learning_rate\n",
        "\n",
        "    if not os.path.exists(args.output):\n",
        "        os.mkdir(args.output)\n",
        "\n",
        "    model = resnet18(custom_config['num_priors'], custom_config['num_classes'])\n",
        "\n",
        "    prior_box_s = prior_boxes(custom_config)\n",
        "    prior_box_s_gpu = prior_box_s.cuda()\n",
        "\n",
        "    overlap_threshold = custom_config['overlap_threshold']\n",
        "    neg_pos_ratio = custom_config['neg_pos_ratio']\n",
        "    variance = custom_config['variance']\n",
        "\n",
        "    criterion = MultiBoxLoss(overlap_threshold, neg_pos_ratio, variance)\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "    scheduler = MultiStepLR(optimizer=optimizer, milestones=args.multistep, gamma=0.2)\n",
        "\n",
        "    best_loc_loss, best_cls_loss, best_loss = np.inf, np.inf, np.inf\n",
        "    train_loss_s, eval_loss_s = [], []\n",
        "    for epoch in tqdm(list(range(args.epochs))):\n",
        "        # Train model\n",
        "        train_loc_loss, train_cls_loss, train_loss = 0, 0, 0\n",
        "        model.train()\n",
        "        for i, (image_s_cpu, box_ss_cpu, label_ss_cpu) in enumerate(train_dataloader):\n",
        "            if len(box_ss_cpu) > 0 and len(label_ss_cpu) > 0:\n",
        "                image_s_gpu = image_s_cpu.cuda()\n",
        "                label_ss_gpu = [label_s_cpu.cuda() for label_s_cpu in label_ss_cpu]\n",
        "                box_ss_gpu = [box_s_cpu.cuda() for box_s_cpu in box_ss_cpu]\n",
        "\n",
        "                pred_loc_ss_gpu, pred_conf_ss_gpu = model(image_s_gpu)\n",
        "\n",
        "                loc_loss, cls_loss = criterion(\n",
        "                    (pred_loc_ss_gpu, pred_conf_ss_gpu, prior_box_s_gpu), (label_ss_gpu, box_ss_gpu)\n",
        "                )\n",
        "\n",
        "                loss = loc_loss + cls_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loc_loss += loc_loss.item()\n",
        "                train_cls_loss += cls_loss.item()\n",
        "                train_loss += loss.item()\n",
        "        scheduler.step()\n",
        "        train_loss_s.append(train_loss)\n",
        "\n",
        "        # Eval model\n",
        "        eval_loc_loss, eval_cls_loss, eval_loss = 0, 0, 0\n",
        "        model.eval()\n",
        "\n",
        "        # Data for mAP\n",
        "        pred_boxes_all, pred_scores_all, pred_labels_all = [], [], []\n",
        "        gt_boxes_all, gt_labels_all = [], []\n",
        "\n",
        "        for i, (image_s_cpu, box_ss_cpu, label_ss_cpu) in enumerate(test_dataloader):\n",
        "            if len(box_ss_cpu) > 0 and len(label_ss_cpu) > 0:\n",
        "                image_s_gpu = image_s_cpu.cuda()\n",
        "                label_ss_gpu = [label_s_cpu.cuda() for label_s_cpu in label_ss_cpu]\n",
        "                box_ss_gpu = [box_s_cpu.cuda() for box_s_cpu in box_ss_cpu]\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    pred_loc_ss_gpu, pred_conf_ss_gpu = model(image_s_gpu)\n",
        "\n",
        "                # Convert predictions to boxes, scores, and labels\n",
        "                pred_boxes, pred_labels, pred_scores = detect_objects(\n",
        "                    pred_loc_ss_gpu.cpu(), pred_conf_ss_gpu.cpu(), prior_box_s,\n",
        "                    custom_config['num_classes'], 0.5, 0.3\n",
        "                )\n",
        "\n",
        "                pred_boxes_all.extend(pred_boxes)\n",
        "                pred_scores_all.extend(pred_scores)\n",
        "                pred_labels_all.extend(pred_labels)\n",
        "                gt_boxes_all.extend(box_ss_cpu)\n",
        "                gt_labels_all.extend(label_ss_cpu)\n",
        "\n",
        "                # Calculate loss for evaluation\n",
        "                loc_loss, cls_loss = criterion(\n",
        "                    (pred_loc_ss_gpu, pred_conf_ss_gpu, prior_box_s_gpu), (label_ss_gpu, box_ss_gpu)\n",
        "                )\n",
        "                loss = loc_loss + cls_loss\n",
        "\n",
        "                eval_loc_loss += loc_loss.item()\n",
        "                eval_cls_loss += cls_loss.item()\n",
        "                eval_loss += loss.item()\n",
        "        eval_loss_s.append(eval_loss)\n",
        "\n",
        "\n",
        "        map_score = mAP(custom_config, pred_boxes_all, pred_scores_all, gt_boxes_all,pred_labels, gt_labels_all)\n",
        "\n",
        "\n",
        "        print(\n",
        "            f'epoch[{epoch}] | lr {scheduler.get_last_lr()[0]:.5f} | '\n",
        "            f'loc_loss [{train_loc_loss:.2f}/{eval_loc_loss:.2f}] | '\n",
        "            f'cls_loss [{train_cls_loss:.2f}/{eval_cls_loss:.2f}] | '\n",
        "            f'total_loss [{train_loss:.2f}/{eval_loss:.2f}] | mAP {map_score:.4f}'\n",
        "        )\n",
        "\n",
        "        if eval_loss < best_loss:\n",
        "            torch.save(model.state_dict(), os.path.join(args.output, f\"{custom_config['model_name']}.pth\"))\n",
        "            best_loc_loss, best_cls_loss, best_loss = eval_loc_loss, eval_cls_loss, eval_loss\n",
        "\n",
        "    return model, prior_box_s, train_loss_s, eval_loss_s\n"
      ],
      "metadata": {
        "id": "ZKYtFgSy1Knb"
      },
      "id": "ZKYtFgSy1Knb",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9ab05aff-93c3-45c2-801c-6b6cf96e73da",
      "metadata": {
        "id": "9ab05aff-93c3-45c2-801c-6b6cf96e73da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065cdd33-28db-4d12-e7df-a3da678f174d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 1/15 [02:35<36:19, 155.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[0] | lr 0.00100 | loc_loss [399.69/11.99] | cls_loss [748.19/25.61] | total_loss [1147.88/37.60] | mAP 0.1039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [04:35<29:09, 134.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[1] | lr 0.00100 | loc_loss [188.16/6.28] | cls_loss [438.25/14.68] | total_loss [626.41/20.97] | mAP 0.3089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [06:30<25:09, 125.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[2] | lr 0.00100 | loc_loss [123.31/4.31] | cls_loss [292.08/12.15] | total_loss [415.39/16.47] | mAP 0.2850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [08:08<21:02, 114.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[3] | lr 0.00100 | loc_loss [98.13/3.80] | cls_loss [268.98/11.47] | total_loss [367.10/15.27] | mAP 0.2795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [09:37<17:35, 105.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[4] | lr 0.00100 | loc_loss [83.44/3.32] | cls_loss [245.11/11.22] | total_loss [328.55/14.54] | mAP 0.2423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [11:00<14:40, 97.80s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[5] | lr 0.00100 | loc_loss [71.21/3.15] | cls_loss [229.85/11.32] | total_loss [301.06/14.46] | mAP 0.2821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [12:18<12:08, 91.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[6] | lr 0.00100 | loc_loss [65.54/3.04] | cls_loss [218.11/10.47] | total_loss [283.65/13.51] | mAP 0.2516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [13:29<09:53, 84.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[7] | lr 0.00100 | loc_loss [61.17/2.65] | cls_loss [208.14/10.16] | total_loss [269.31/12.80] | mAP 0.2138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [14:44<08:10, 81.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[8] | lr 0.00100 | loc_loss [57.26/3.08] | cls_loss [200.80/10.25] | total_loss [258.06/13.33] | mAP 0.2536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [15:54<06:30, 78.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[9] | lr 0.00100 | loc_loss [53.80/2.71] | cls_loss [194.18/9.64] | total_loss [247.98/12.35] | mAP 0.2009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [17:02<04:59, 74.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[10] | lr 0.00100 | loc_loss [51.85/2.69] | cls_loss [185.76/9.62] | total_loss [237.60/12.30] | mAP 0.2352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [18:04<03:33, 71.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[11] | lr 0.00100 | loc_loss [48.32/2.45] | cls_loss [181.20/9.72] | total_loss [229.52/12.17] | mAP 0.2578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [19:10<02:19, 69.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[12] | lr 0.00100 | loc_loss [46.73/2.67] | cls_loss [178.28/10.12] | total_loss [225.01/12.80] | mAP 0.2430\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [20:04<01:04, 64.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[13] | lr 0.00100 | loc_loss [44.19/2.52] | cls_loss [170.36/9.63] | total_loss [214.55/12.15] | mAP 0.2254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [20:57<00:00, 83.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[14] | lr 0.00100 | loc_loss [42.51/2.39] | cls_loss [165.08/9.42] | total_loss [207.60/11.81] | mAP 0.2600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from argparse import Namespace\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "\n",
        "param_s = Namespace(\n",
        " dataset_root_dir='dataset',\n",
        " epochs = 15, batch_size = 8,\n",
        " checkpoint = None, output = 'output',\n",
        " multistep = [20, 30, 40],\n",
        " learning_rate = 1e-3, momentum = 0.9,\n",
        " weight_decay = 0.0005, warmup = None,\n",
        " num_workers = 4,\n",
        " seed = 42\n",
        ")\n",
        "\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "custom_config = {\n",
        " 'num_classes'  : 3,\n",
        " 'feature_maps' : [(23, 40), (12, 20), (6, 10), (3, 5), (2, 3), (1, 2)], #640x360\n",
        " 'min_sizes'    : [0.10, 0.20, 0.3, 0.4, 0.7, 1.00],\n",
        " 'max_sizes'    : [0.20, 0.3, 0.4, 0.7, 1.00, 1.1],\n",
        "\n",
        " 'aspect_ratios': [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]],\n",
        " 'num_priors'   : [6, 6, 6, 6, 6, 6],\n",
        " 'variance'     : [0.1, 0.2],\n",
        " 'clip'         :    True,\n",
        "\n",
        " 'overlap_threshold': 0.5,\n",
        " 'neg_pos_ratio'    :   3,\n",
        "\n",
        " 'model_name' : 'resnet18'\n",
        "}\n",
        "\n",
        "model, prior_box_s, train_loss_s, eval_loss_s = train_resnet(param_s, custom_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим значительное улучшение метрики МАП"
      ],
      "metadata": {
        "id": "Jzw9oI8_Jgn_"
      },
      "id": "Jzw9oI8_Jgn_"
    },
    {
      "cell_type": "markdown",
      "id": "6a0567ae-804c-4306-8dd1-59d400109d97",
      "metadata": {
        "id": "6a0567ae-804c-4306-8dd1-59d400109d97"
      },
      "source": [
        "## (БОНУС) Добавить разнообразные аугментации изображений в классе Dataset. Провести эксперименты и продемонстрировать метрику mAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2613cd-b96a-430c-abfe-0448bb60dc5a",
      "metadata": {
        "id": "ea2613cd-b96a-430c-abfe-0448bb60dc5a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}